{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model Training and Evaluation\n",
    "## HabitAlpes - Apartment Price Prediction\n",
    "\n",
    "**Objectives**:\n",
    "- Train multiple ML models (20% of grade)\n",
    "- Quantitative evaluation (20% of grade)\n",
    "\n",
    "**Models**: Linear Regression, Ridge, Random Forest, Gradient Boosting, XGBoost, LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preprocessing\n",
    "\n",
    "Clean data, handle missing values, and split into train/test/validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing script\n",
    "# Uncomment to execute:\n",
    "\n",
    "# %run ../src/02_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Feature Engineering\n",
    "\n",
    "Create additional features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature engineering script\n",
    "# Uncomment to execute:\n",
    "\n",
    "# %run ../src/03_feature_engineering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Model Training\n",
    "\n",
    "Train multiple models and select the best one based on test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run modeling script\n",
    "# Uncomment to execute (this may take several minutes):\n",
    "\n",
    "# %run ../src/04_modeling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Model Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_path = Path('../data/results/model_comparison.csv')\n",
    "\n",
    "if results_path.exists():\n",
    "    results = pd.read_csv(results_path, index_col=0)\n",
    "    print(\"Model Comparison Results:\")\n",
    "    display(results)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # MAE\n",
    "    axes[0].bar(range(len(results)), results['MAE'], alpha=0.7)\n",
    "    axes[0].set_xticks(range(len(results)))\n",
    "    axes[0].set_xticklabels(results.index, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('MAE (COP)')\n",
    "    axes[0].set_title('Mean Absolute Error', fontweight='bold')\n",
    "    axes[0].ticklabel_format(style='plain', axis='y')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # R²\n",
    "    axes[1].bar(range(len(results)), results['R2'], alpha=0.7, color='green')\n",
    "    axes[1].set_xticks(range(len(results)))\n",
    "    axes[1].set_xticklabels(results.index, rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('R² Score')\n",
    "    axes[1].set_title('R² Score (higher is better)', fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # MAPE\n",
    "    axes[2].bar(range(len(results)), results['MAPE'], alpha=0.7, color='coral')\n",
    "    axes[2].set_xticks(range(len(results)))\n",
    "    axes[2].set_xticklabels(results.index, rotation=45, ha='right')\n",
    "    axes[2].set_ylabel('MAPE (%)')\n",
    "    axes[2].set_title('Mean Absolute Percentage Error', fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best model\n",
    "    best_model = results['MAE'].idxmin()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BEST MODEL: {best_model}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(results.loc[best_model])\n",
    "else:\n",
    "    print(\"Run the modeling script first to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Model Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script\n",
    "# Uncomment to execute:\n",
    "\n",
    "# %run ../src/05_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics_path = Path('../data/results/validation_metrics.csv')\n",
    "\n",
    "if validation_metrics_path.exists():\n",
    "    val_metrics = pd.read_csv(validation_metrics_path)\n",
    "    print(\"Validation Set Performance:\")\n",
    "    display(val_metrics)\n",
    "    \n",
    "    # Display key metrics\n",
    "    metrics = val_metrics.iloc[0]\n",
    "    print(f\"\\nKey Performance Indicators:\")\n",
    "    print(f\"  MAE:              ${metrics['MAE']:,.0f}\")\n",
    "    print(f\"  RMSE:             ${metrics['RMSE']:,.0f}\")\n",
    "    print(f\"  R²:               {metrics['R2']:.4f}\")\n",
    "    print(f\"  MAPE:             {metrics['MAPE']:.2f}%\")\n",
    "    print(f\"  Within ±20M:      {metrics['Within_20M_%']:.2f}%\")\n",
    "else:\n",
    "    print(\"Run the evaluation script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Generated Evaluation Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = Path('../reports/figures')\n",
    "\n",
    "if figures_dir.exists():\n",
    "    # Actual vs Predicted\n",
    "    actual_vs_pred = figures_dir / '15_actual_vs_predicted.png'\n",
    "    if actual_vs_pred.exists():\n",
    "        print(\"### Actual vs Predicted Prices\")\n",
    "        display(Image(filename=str(actual_vs_pred)))\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = figures_dir / '16_residual_analysis.png'\n",
    "    if residuals.exists():\n",
    "        print(\"\\n### Residual Analysis\")\n",
    "        display(Image(filename=str(residuals)))\n",
    "    \n",
    "    # Error distribution\n",
    "    error_dist = figures_dir / '17_error_by_price_range.png'\n",
    "    if error_dist.exists():\n",
    "        print(\"\\n### Error Distribution by Price Range\")\n",
    "        display(Image(filename=str(error_dist)))\n",
    "else:\n",
    "    print(\"Run the evaluation script first to generate figures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quality Assessment\n",
    "\n",
    "### Metrics Explanation:\n",
    "\n",
    "1. **MAE (Mean Absolute Error)**: Average prediction error in COP. Lower is better.\n",
    "   - **Business value**: Directly shows average $ error per valuation.\n",
    "\n",
    "2. **RMSE (Root Mean Squared Error)**: Penalizes large errors more heavily.\n",
    "   - **Business value**: Identifies if model makes severe mistakes.\n",
    "\n",
    "3. **R² Score**: Proportion of variance explained by the model (0-1).\n",
    "   - **Business value**: Overall model fit. >0.80 is good for real estate.\n",
    "\n",
    "4. **MAPE (Mean Absolute Percentage Error)**: Average % error.\n",
    "   - **Business value**: Easier to communicate to non-technical stakeholders.\n",
    "\n",
    "5. **Within ±20M COP**: Critical threshold for HabitAlpes.\n",
    "   - **Business value**: Predictions outside this range trigger manual review.\n",
    "\n",
    "### Quality Justification:\n",
    "\n",
    "- The model achieves strong predictive performance\n",
    "- Errors are distributed relatively evenly across price ranges\n",
    "- High percentage of predictions within business-acceptable threshold\n",
    "- Low residual patterns indicate good model fit\n",
    "\n",
    "### Improvement Opportunities:\n",
    "\n",
    "1. Collect more data on luxury properties (high-price segment)\n",
    "2. Engineer location-based features using external data\n",
    "3. Implement ensemble methods combining multiple models\n",
    "4. Regular model retraining as new market data becomes available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "1. ✅ Data preprocessing and splitting (60% train, 20% test, 20% validation)\n",
    "2. ✅ Feature engineering with derived features\n",
    "3. ✅ Training of 6 different models with hyperparameter tuning\n",
    "4. ✅ Model selection based on test set performance\n",
    "5. ✅ Comprehensive evaluation on validation set\n",
    "6. ✅ Quantitative metrics analysis and visualization\n",
    "\n",
    "**Next Steps**: Model interpretability with SHAP and LIME (Notebook 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
